#!/usr/bin/env python3
"""
æµ‹è¯•æ–°è®­ç»ƒçš„é«˜å°”å¤«æ£€æµ‹æ¨¡å‹
éªŒè¯æ¨¡å‹åŠ è½½å’ŒåŸºæœ¬æ¨ç†åŠŸèƒ½
"""

import sys
import os
import time
import numpy as np
import cv2

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('/home/huxiaoran/projects/MyGolfTracker')

def test_model_loading():
    """æµ‹è¯•æ¨¡å‹åŠ è½½"""
    print("ğŸ” æµ‹è¯•æ¨¡å‹åŠ è½½...")
    
    try:
        from ultralytics import YOLO
        
        model_path = "data/best.pt"
        print(f"ğŸ“ æ¨¡å‹è·¯å¾„: {model_path}")
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(model_path):
            print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
            return False
            
        # è·å–æ–‡ä»¶å¤§å°
        file_size = os.path.getsize(model_path) / (1024 * 1024)  # MB
        print(f"ğŸ“Š æ¨¡å‹æ–‡ä»¶å¤§å°: {file_size:.1f} MB")
        
        # åŠ è½½æ¨¡å‹
        print("â³ åŠ è½½æ¨¡å‹ä¸­...")
        start_time = time.time()
        model = YOLO(model_path)
        load_time = time.time() - start_time
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼Œè€—æ—¶: {load_time:.2f}ç§’")
        
        # æ£€æŸ¥æ¨¡å‹ä¿¡æ¯
        print(f"ğŸ“‹ æ¨¡å‹ç±»åˆ«: {model.names}")
        print(f"ğŸ”¢ ç±»åˆ«æ•°é‡: {len(model.names)}")
        
        return model
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return None

def test_gpu_inference(model):
    """æµ‹è¯•GPUæ¨ç†"""
    print("\nğŸ® æµ‹è¯•GPUæ¨ç†...")
    
    try:
        # æ£€æŸ¥CUDAå¯ç”¨æ€§
        import torch
        if not torch.cuda.is_available():
            print("âš ï¸ CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPUæ¨ç†")
            return False
            
        print(f"ğŸ”§ CUDAè®¾å¤‡: {torch.cuda.get_device_name(0)}")
        
        # åˆ›å»ºæµ‹è¯•å›¾åƒ
        test_image = np.random.randint(0, 255, (640, 640, 3), dtype=np.uint8)
        print(f"ğŸ–¼ï¸ æµ‹è¯•å›¾åƒå°ºå¯¸: {test_image.shape}")
        
        # æµ‹è¯•æ¨ç†
        print("â³ æ‰§è¡Œæ¨ç†...")
        start_time = time.time()
        
        # å°†æ¨¡å‹ç§»åŠ¨åˆ°GPU
        model.to("cuda")
        
        # æ‰§è¡Œæ¨ç†
        results = model(test_image, conf=0.01, iou=0.7, max_det=10)
        
        inference_time = time.time() - start_time
        print(f"âœ… æ¨ç†å®Œæˆï¼Œè€—æ—¶: {inference_time:.3f}ç§’")
        
        # æ£€æŸ¥ç»“æœ
        if results and len(results) > 0:
            result = results[0]
            detections = len(result.boxes) if result.boxes is not None else 0
            print(f"ğŸ¯ æ£€æµ‹åˆ° {detections} ä¸ªç›®æ ‡")
            
            if detections > 0:
                confidences = result.boxes.conf.cpu().numpy()
                print(f"ğŸ“Š ç½®ä¿¡åº¦èŒƒå›´: {confidences.min():.3f} - {confidences.max():.3f}")
        
        return True
        
    except Exception as e:
        print(f"âŒ GPUæ¨ç†æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_model_performance(model):
    """æµ‹è¯•æ¨¡å‹æ€§èƒ½"""
    print("\nğŸ“ˆ æµ‹è¯•æ¨¡å‹æ€§èƒ½...")
    
    try:
        # åˆ›å»ºå¤šä¸ªæµ‹è¯•å›¾åƒ
        test_images = []
        for i in range(5):
            img = np.random.randint(0, 255, (640, 640, 3), dtype=np.uint8)
            test_images.append(img)
        
        print(f"ğŸ–¼ï¸ å‡†å¤‡ {len(test_images)} å¼ æµ‹è¯•å›¾åƒ")
        
        # æ‰¹é‡æ¨ç†æµ‹è¯•
        start_time = time.time()
        
        for i, img in enumerate(test_images):
            results = model(img, conf=0.01, iou=0.7, max_det=10, verbose=False)
            
        total_time = time.time() - start_time
        avg_time = total_time / len(test_images)
        
        print(f"âœ… æ‰¹é‡æ¨ç†å®Œæˆ")
        print(f"â±ï¸ æ€»è€—æ—¶: {total_time:.3f}ç§’")
        print(f"ğŸ“Š å¹³å‡æ¯å¼ : {avg_time:.3f}ç§’")
        print(f"ğŸš€ æ¨ç†é€Ÿåº¦: {1/avg_time:.1f} FPS")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æµ‹è¯•æ–°è®­ç»ƒçš„é«˜å°”å¤«æ£€æµ‹æ¨¡å‹")
    print("=" * 50)
    
    # æµ‹è¯•æ¨¡å‹åŠ è½½
    model = test_model_loading()
    if model is None:
        print("âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œæµ‹è¯•ç»ˆæ­¢")
        return False
    
    # æµ‹è¯•GPUæ¨ç†
    gpu_success = test_gpu_inference(model)
    
    # æµ‹è¯•æ€§èƒ½
    perf_success = test_model_performance(model)
    
    print("\n" + "=" * 50)
    print("ğŸ“‹ æµ‹è¯•æ€»ç»“:")
    print(f"âœ… æ¨¡å‹åŠ è½½: æˆåŠŸ")
    print(f"{'âœ…' if gpu_success else 'âŒ'} GPUæ¨ç†: {'æˆåŠŸ' if gpu_success else 'å¤±è´¥'}")
    print(f"{'âœ…' if perf_success else 'âŒ'} æ€§èƒ½æµ‹è¯•: {'æˆåŠŸ' if perf_success else 'å¤±è´¥'}")
    
    if gpu_success and perf_success:
        print("\nğŸ‰ æ–°æ¨¡å‹æµ‹è¯•é€šè¿‡ï¼å¯ä»¥éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒã€‚")
        return True
    else:
        print("\nâš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®ã€‚")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
